\documentclass[report.tex]{subfiles}
\begin{document}

\chapter{Implementation} % (fold)
\label{cha:implementation}

% ==============================================================================
% ==============================================================================

\section{Implementation Languages} % (fold)
\label{sec:implementation_languages}

\subsection{Java 8} % (fold)
\label{sub:java_8}
The compiler was created using Java version 8. The primary motivation behind
using Java was my familiarity with the language and the libraries \& frameworks
available. The complex nature of this project meant that I did not want to spend
time getting familiar with a language such as C++, while I knew that I take
advantage of a build system like Maven to handle the inevitable dependency and
testing suites I would require.
% subsection java_8 (end)

\subsection{PRISM} % (fold)
\label{sub:prism}
PRISM (\url{http://prismmodelchecker.org}) was naturally required for the
implementation of the compiler. It was necessary to ensure that the models being
generated by the compiler were valid and were accurately reflecting the set-up
specified in a .qgrady file.
% subsection prism (end)

% ==============================================================================

% subsection subsection_name (end)
% section implementation_languages (end)
\section{Tools} % (fold)
\label{sec:tools}
The Q'Grady compiler makes use of several libraries to assist in development.
These have been outlined below, outlining their function and why they were used
for the development of the compiler.

\subsection{CUP} % (fold)
\label{sub:cup}
CUP (\url{http://www2.cs.tum.edu/projects/cup}) is a parser generator that was
used to convert the Q'Grady grammar as specified in the .cup file into Java
classes that could be inserted into the Q'Grady compiler. CUP was suggested by
my supervisor, and looking at the documentation showed that it would be a
strong choice.
% subsection cup (end)

\subsection{JFlex} % (fold)
\label{sub:jflex}
JFlex (\url{http://jflex.de})is a scanner generator for Java. It generated an
additional Java class for the compiler, matching tokens from the scanned input
to regular expressions outlined in the .flex file. JFlex is specifically
designed for use with CUP, making it an easy choice to use.
% subsection jflex (end)

\subsection{Apache Maven} % (fold)
\label{sub:apache_maven}
Maven (\url{http://maven.apache.org}) was used primarily for handling dependency
injection. Plugins for JFlex and CUP were used to make the use of the libraries
a lot easier, since they would handle executing the generation of classes while
compiling my own Java classes. It also allowed me to automate JUnit testing
during the compilation and packaging stages.
% subsection apache_maven (end)

\subsection{Apache Commons CLI \& IO} % (fold)
\label{sub:apache_commons_cli}
The Apache Commons CLI (\url{http://commons.apache.org/proper/commons-cli}) was
used for handling the parsing of command line arguments when executing the
compiler. It was used for handling the logic behind managing the presence of
the mandatory and optional arguments for the compiler, such as the source and
destination files.

The Commons IO (\url{http://commons.apache.org/proper/commons-io}) library was
required for methods relating to handling files, such as ensuring that files
had the proper extension when required.
% subsection apache_commons_cli (end)

\subsection{JUnit} % (fold)
\label{sub:junit}
JUnit (\url{http://junit.org}) was used for testing the Java code, ensuring that
features of the compiler were accurate and doing what was intended.
% subsection junit (end)
% section tools (end)

% ==============================================================================

\section{The Q'Grady Application} % (fold)
\label{sec:the_q_grady_application}
Before I designed the Q'Grady languages itself, I had laid out the general
control flow of the application. The \texttt{Q'Grady} class is the main point of
entry of the compiler, which goes through each of the stages laid out in Section
\ref{sub:application}.

The system starts by using the Apache Commons CLI library to analyse the
program arguments provided by the user, to determine the course of actions. If
the user wants help or other information, it will display the desired
information to the user. Similar to compilers like GCC, the Q'Grady compiler
does have a priority in terms of options, as follows.
\begin{enumerate}
    \item The `Help' command.
    \item The `Version' command.
    \item Actual compilation.
\end{enumerate}

In the compilation stage, the source file will be passed to the parser generated
by CUP (see \ref{sub:cup_impl}) which produces a \texttt{Box} object. The
application then goes through all of the \texttt{SemanticAnalyser}'s methods to
ensure the validity of the box before passing the box to a new
\texttt{FileGenerator} object to write the PRISM model. 

Caught exceptions will display the appropriate message to the user, and then
exit the system. This is because the exceptions will most likely indicate that
changes must be made to the original source file to ensure the application can
progress.
% section the_q_grady_application (end)

\section{The Box Class} % (fold)
\label{sec:the_box_class}
The first problem to tackle as part of the compiler was an obvious one: how to
store the box set-up as a Java object to be used throughout the compilation
process. This included the storage of the probability distribution, the inputs
and outputs required, and all the methods required for calculating the other
probabilities.

\subsection{Storing the Distribution} % (fold)
\label{sub:storing_the_distribution}
The first method of storing the distribution was used when the Q'Grady language
had no way of showing the number of inputs/outputs or their ranges, it had to be
assumed based on the size of the matrix. Thus, the initial storage was a
\texttt{Map} of \texttt{Instance}s to the probability of that \texttt{Instance}.

The \texttt{Instance} consisted of two arrays that were the input and output
values of a possible outcome. As the language was extended to incorporate
the new features, this method was replaced with the more simple solution of
simply storing the matrix parsed by the compiler as a two-dimensional array
instead. The Instance class felt rather clumsy to work with, and simply using
the two-dimensional array with conversion methods instead was seen as a better
alternative. The reduced probability method for example, was a lot more
confusing working with the maps.

The variables of the box's input and output are stored as two \texttt{List}s,
with the \texttt{SemanticAnalyser} ensuring there are no issues with these
lists (see Section \ref{ssub:validateVariables}).
% subsection storing_the_distribution (end)

\subsection{Reduced Probability} % (fold)
\label{sub:reduced_probability}
As touched upon in Section \ref{sub:non_signalling}, the reduced probability is
the probability of a single output value given a single input value, requiring
a way to ignore the other values.

\lstinputlisting[
    numbers = left,
    basicstyle = \ttfamily\footnotesize,
    frame = single,
    language = Java,
    breaklines = true,
    caption = {\texttt{Box} reduced probability},
    label={lst:reduced_method}
]{files/reduced.java} 

The method shown in Listing \ref{lst:reduced_method} is used to obtain the
reduced probabilities from the \texttt{Box} class, taking in the index of
two variables and their values.

It starts by systematically going through every possible input and output, and
accumulating the probabilities of all that match the given input and output
based on the index and value provided. For example, the call \(prob(0, 0, 0, 0\)
would retrieve all probabilities in the matrix where \(x = 0\) and \(a = 0\)
for the \texttt{pr.qgrady} file in Listing \ref{lst:pr_qgrady_example}

It then divided that sum by \(inputRange ^{(inputs.size() - 1)}\) to provide the
reduced probability. This method means that it is flexible for set-ups beyond
that of the basic boxes such as the PR box. This division is required to ensure
that the values remain accurate, otherwise it is most likely that sum over the
range of values of the output will not equal 1 like it should.

When the \texttt{Instance} class was used as part of the box, this method was
extremely more complicated, although that may have been due to poor design of
the algorithm rather than an inherent flaw of that storage method itself. It
involved conversions from \texttt{Map}s to \texttt{Set}s that was too confusing
to really understand what was happening.
% subsection reduced_probability (end)

\subsection{Normalised Probability} % (fold)
\label{sub:normalised_probability}
The normalised probability saw difficulties in implementing the handling of more
than two variables. It is very simple to handle the case were all but one output
is known (and the same for handling a single output like in the reduced
probabilities). Indeed, attempts to change the normalising method to handle the
use case where the file generator is handling the stages where the second
output's probabilities are being normalised after deciding the first output is
decided.

\lstinputlisting[
    numbers = left,
    basicstyle = \ttfamily\footnotesize,
    frame = single,
    language = Java,
    caption = {Normalised Probability method.}, 
    label={lst:normalised_method}
]{files/normalised.java} 

% subsection normalised_probability (end)

\subsection{Helper Methods} % (fold)
\label{sub:helper_methods}
Two static helper methods were required to allow for integer to array
conversion, flexible to handle any base that acted as the range for inputs and
outputs. These were already necessary for the file generation and other parts
of the system, meaning their inclusion was not a major problem, and was in fact
straight forward.

Getters were also provided for all the ranges and variable lists, due to their
necessity in the file generation and semantics checking for the compiler.
% subsection helper_methods (end)
% section the_box_class (end)

\section{Syntax Checking} % (fold)
\label{sec:syntax_checking}
When the \texttt{Box} class was decided upon, the next stage in the
implementation was the parsing of Q'Grady files to be stored as a \texttt{Box}
object.

This was achieved through the Cup and JFlex libraries. The two libraries would
create Java classes that would use the given specifications to create a parser
and lexer that could be used in the compiler.

\subsection{CUP Implementation} % (fold)
\label{sub:cup_impl}
I started with the parser generator using Cup. The Cup file used a modification
of the Q'Grady grammar, based on examples in Cup's documentation 
\cite{cup_example}. The lists required for variables and probabilities to be
handled in a specific way that I decided to use the MiniJava example from CUP
as a standard to work with, knowing that it was going to work. The error
handling did not change from the CUP examples either, due to an uncertainty
about how to best handle it.
% subsection cup_impl (end)

\subsection{JFlex Implementation} % (fold)
\label{sub:jflex_impl}
The lexer was created using JFlex, which allowed for the pairing of regular
expressions with symbols based on CUP's processing of the CUP file, which would
extract all the terminal symbols, and provide a symbols class that could be
used. Listings \ref{lst:qgrady_flex} provides a snippet of the JFlex file, which
demonstrates how characters are matched to terminal symbols in the CUP file.

\lstinputlisting[numbers=left, basicstyle=\ttfamily\footnotesize,
caption={qgrady.flex}, captionpos=b, label={lst:qgrady_flex},
frame=single, breaklines=true]{files/qgrady.flex} 

% subsection jflex_impl (end)

However, the error handling of the generated files is not excellent. The system
does not provide the best information about why the source file failed to parse,
not altering the user to missing semi-colons for example. In addition, reading
the code of the generated Java classes proved to be an unwise decision, due to
the inevitable spaghetti code that arises from such a process (a fate that I
would soon realise myself).

On the other hand, I did find the two libraries straight forward to work with
after learning the provided examples. The availability of Maven plugins also
helped greatly during the development process, since it allowed for changes to
the CUP/JFlex files to be handled as part of the compilation of all Java
classes, making the process much easier.
% section syntax_checking (end)

\section{Semantics Analysing} % (fold)
\label{sec:semantics_analysing}
I decided that the \texttt{SemanticsAnalyser} class would be a series of static
methods, each taking a \texttt{Box} or its probability array as a parameter,
since I felt that a constructed object was not necessary for functionality. The
methods ranged from straight forward (\texttt{validateValues}) to the much more
complex (\texttt{nonSignalling}).

\subsubsection{ValidateVariables} % (fold)
\label{ssub:validateVariables}
The \texttt{ValidateVariables} method goes through every variable in the input
and output lists, ensuring that the given criteria is met. This was a simple
check to add, with the most challenging part being finding all the keywords of
the PRISM language, which was found in part of the online manual
\cite{prism_keywords}.
\begin{itemize}
    \item PRISM keywords does not contain variable.
    \item Variable does not appear multiple times in a list.
    \item Variable does not appear in both lists.
\end{itemize}
% subsubsection validateVariables (end)

\subsubsection{ValidateValues} % (fold)
\label{ssub:validatevalues}
The \texttt{ValidateValues} method simply goes through the probabilities array
and ensures all values are between 0 and 1 inclusively.
% subsubsection validatevalues (end)

\subsubsection{ValidateRowAmount} % (fold)
\label{ssub:validaterowamount}
The \texttt{ValidateRowAmount} method looks at the length of the probability
array, the number of rows it has. The method compares this to the expected
value of \(range ^{inputs}\), where range is the range of values an input can be
and inputs is the number of input variables declared. For example, the PR box
should have 4 rows, since there are 2 input variables with a range of 2.
% subsubsection validaterowamount (end)

\subsubsection{ValidateRowLengths} % (fold)
\label{ssub:validaterowlengths}
\texttt{ValidateRowLengths} is essentially the same as 
\texttt{ValidateRowAmount} except it uses the output data rather than the input
data. It goes through each row of the array and does a comparison between each
of the row lengths and the expected value of \(range ^{outputs}\).
% subsubsection validaterowlengths (end)

\subsubsection{ValidateRowSums} % (fold)
\label{ssub:validaterowsums}
The nature of the probability array means that every row is the probability of
output given that row's specific input. For example, \texttt{probs[0]} of the
PR box would be the input 00 (based on the \texttt{intToBitArray} method of the
\texttt{Box} class). Each row's probabilities must sum up to 1 in order for the
resulting PRISM model to work.

When implementing this particular check, there was the realization that since
the language uses decimal numbers rather than fractions, it did not handle
fractions like \(\frac{1}{3}\) nicely. Tweaking of PRISM models found that five
decimal places was the sweet spot where this issue was handled, so the check
had a tolerance for this inserted into it during the future states of
development.
% subsubsection validaterowsums (end)

\subsection{Non-signalling} % (fold)
\label{sub:non_signalling_impl}
The \texttt{nonSignalling} method is the most important method of the
\texttt{SemanticAnalyser} class. It is vital to ensure that the non-signalling
check is maintained in the set-up being modelled. Refer back to Section
\ref{sub:non_signalling} for its definition.

The earliest non-signalling check was specific to PR box-like models, with two
inputs and outputs each dealing with binary digits. This resulted in an
undesired nested loop that did not look nice at all.

\lstinputlisting[numbers=left, basicstyle=\ttfamily\footnotesize,
caption={The first non-signalling check}, captionpos=b, frame=single,
label={lst:nonsignalling1}, breaklines=true]{files/nonsignalling1.txt}

During development I made the initial mistake of only adding the check on one
side, without the other possible signalling possibilities being examined. This
lead to a useless check that had to be redesigned in the first place.

A new non-signalling check was added, but it was still not performing to the
desired level. There were issues in cases where there were more than two
variables that had to be examined.

\lstinputlisting[numbers=left, basicstyle=\ttfamily\footnotesize,
caption={The final non-signalling check}, captionpos=b, frame=single,
label={lst:nonsignalling2}, breaklines=true]{files/nonsignalling2.txt}

The final non-signalling check saw it split into two methods for ease of use.
The top algorithm would go through all the inputs and outputs that remain
constant through the non-signalling check (this would be b and y in Equation
\ref{eq:non-signal_1}), while the second method handles the summation and
changing of the output, based on the given index being looked at.

The second algorithm inserts a new value into both of the given arrays, which
are the ones updated as shown in the equation. The final check on the sums
array ensures that all the summations are equal. This means that the cases where
the input has a wide range does not have a large impact on performance as
previous attempts at non-signalling did.

The check has use the same index for inputs and outputs, meaning that set-ups
where the number of inputs and outputs differ may result in potential
complications. There was unfortunately not enough time to investigate this
further. The non-signalling equations paired an input with an output, so it was
seen as an unavoidable situation to deal with.

Aside from this potential issue, I do believe that this was the best way to
approach the non-signalling check that I could accomplish in the time of this
project, although I would expect more efficient algorithms could be made to
improve the \texttt{SemanticAnalyser} class.
% subsection non_signalling_impl (end)
% section semantics_analysing (end)

\section{File Generation} % (fold)
\label{sec:file_generation}
\subsection{PrismMacros} % (fold)
\label{sub:prismmacros}
Early implementations of the file generation part of the compiler was done with
the use of a single class \texttt{FileGenerator} (see \ref{sub:filegenerator})
that contained numerous \texttt{String} fields repeatedly used throughout the
class. It became apparent later on that the best source of action was to create
the \texttt{PrismMacro} class, which similarly to \texttt{SemanticAnalyser},
contained multiple public static methods that would produce PRISM language
statements and commands based on what it is fed.

Each method starts with a template string, which has temporary sub-strings
inside that are ready to be replaced. The placeholders are private fields in the
class itself, due to their repeated sightings in these methods. 
\ref{lst:prismmacros} shows an example of how these methods work.

\lstinputlisting[numbers=left, basicstyle=\ttfamily\footnotesize,
caption={The `command' PrismMacro method}, captionpos=b, frame=single,
label={lst:prismmacros}, breaklines=true]{files/prismmacros.txt}

The class allowed for much easier handling of recurring statements in the
produced file, such as the declaration of variables and modules, the assigning
of probabilities to actions, and grouping lists of actions into a single
statement. The fact that all parts of the \texttt{FileGenerator} class used
these methods meant that changes to the layout of the PRISM file became much
easier to deal with.

The \texttt{command} method was the most utilised method in this class. As its
name implies, the method returns a string containing a PRISM statement. This
method takes in this statement's guard, actions and its synchronise label to
produce strings that can easily be added to the new file.

The \texttt{varDec} method has similar benefits. Initially, when the language
only allowed for the values 0 and 1, all this method needed was the name of the
variable being declared. However, the future additions of the feature to
decide the range of inputs and outputs, and the need for a \texttt{ready}
variable in the model, necessitated the extension of this method to take the
range and initial value of the variable into account as well, making it a much
more flexible method to use.
% subsection prismmacros (end)

\subsection{FileGenerator} % (fold)
\label{sub:filegenerator}
The \texttt{FileGenerator} class works through continuously updating an
\texttt{ArrayList} of strings that are eventually read to the destination file.
Its methods are split up to handle the inputs and outputs, with the outputs
requiring further break down to handle the increased complexity compared to the
inputs.

\lstinputlisting[numbers=left, basicstyle=\ttfamily\footnotesize,
caption={Generating input modules}, captionpos=b, frame=single,
label={lst:inputmodules}, breaklines=true]{files/inputfile.txt}

Each input is given it's own module, with the declaration, uniform distribution
of assignment, and the sync actions that project the value of the variable for
the output module's use.

The output is handled differently since it requires one module for all outputs
rather than each having its own module. This results in that implementation
being handled slightly differently, where the implementation (and thus the
generated file) has a specific structure.
\begin{itemize}
    \item Generate output declarations.
    \item Generate output syncs.
    \item Generate reduced probabilities actions.
    \item Generate normalised probabilities actions.
\end{itemize}

\subsubsection{Output Declaration} % (fold)
\label{ssub:output_dec_impl}
The output declarations are handled in a similar way as shown on line  1 of
\ref{lst:inputmodules}, except using the output range rather than the input
range naturally.

The output module's \texttt{ready} variable is also declared at this stage,
the system the output module uses to indicate that it is ready to synchronise
on an input module's action to assign the appropriate output.
% subsubsection output_dec_impl (end)

\subsubsection{Output Syncs} % (fold)
\label{ssub:output_syncs}
The output synchronized actions do have an addition factor to take into
consideration. These syncs have the addition guard that \texttt{ready} be
currently set to 0 before this action can occur.

\lstinputlisting[numbers=left, basicstyle=\ttfamily\footnotesize,
caption={\texttt{FileGenerator.java outputsyncs()}}, captionpos=b, frame=single,
label={lst:outputsyncs}, breaklines=true]{files/outputsyncs.txt}

As \ref{lst:outputsyncs} shows, the additional guard for \texttt{ready} is
added to a \texttt{guards} array, which is converted to a string separated by
the `\&' character to allow for the proper syntax of PRISM being met.
% subsubsection output_syncs (end)

\subsubsection{Reduced Probability} % (fold)
\label{ssub:reduced_probability}
The reduced probability functionality required going through every possible
input for every possible output. The synchronise label would be the input and
its value, matching the labels found in Listing \ref{lst:inputmodule}. The
implemented guard ensures that \texttt{ready} is equal to one and the output
is set to -1, meaning that it is unassigned. The action part of this command is
generated through going thorugh each possible value the output can be, and its
reduced probability (see Listings \ref{lst:reducedprob} and 
\ref{lst:reduced_method}).
% subsubsection reduced_probability (end)

\subsubsection{Normalised Probability} % (fold)
\label{ssub:normalised_probability}
The previous parts of the file generation saw little changes from the initial
attempts, aside from the addition of the \texttt{PrismMacro} class and the
\texttt{ready} variable. The normalised probabilities, on the other hand, were
the most difficult part of the file generation to accomplish. Regrettably, the
final method was not as functional as required. Due to the limitations of
retrieving the normalised probabilities outlined in Section 
\ref{sub:normalised_probability}, the generated lines for more than two output
variables do not match the specification. A lack of time prevented the
possibility of fixing this issue.

This method became extremely complex as a result. It attempts to cover every
possible path that can be followed up until this stage. Given a box such as the
PR box, the box will have the input values and the other output value already
decided at this stage. This is how it can normalise the final probabilities.
The file generator must handle all these cases (which is does for the simple
boxes), but naturally, this leads to less than desirable time complexities
that do not have any obvious way of improving.

In lines 216 and 228 of the \texttt{FileGenerator} class,  there is an addition
and removal to the guard that is required to avoid weird behaviour that was
found in the generation in dealing with the basic boxes. It is certainly clear
that in future improvements to the Q'Grady compiler, a refactoring (or complete
replacement) of this method would be a top priority.
% subsubsection normalised_probability (end)

% subsection filegenerator (end)
% section file_generation (end)
% ==============================================================================
% ==============================================================================
% chapter implementation (end)
\newpage
\end{document}